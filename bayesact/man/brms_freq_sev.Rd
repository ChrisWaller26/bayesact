% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/brms_freq_sev.R
\name{brms_freq_sev}
\alias{brms_freq_sev}
\title{Bayesian Frequency Severity Model with BRMS}
\usage{
brms_freq_sev(
  freq_formula,
  sev_formula,
  freq_family,
  sev_family,
  freq_data,
  sev_data,
  priors,
  ded_name = "ded",
  freq_adj_fun,
  ...
)
}
\arguments{
\item{freq_formula}{BRMS Formula; Linear/Non-linear formula for frequency model}

\item{sev_formula}{BRMS Formula; Linear/Non-linear formula for severity model}

\item{freq_family}{Family; Family for frequency model}

\item{sev_family}{Family; Family for severity model}

\item{freq_data}{Data Frame; The data required for the frequency model}

\item{sev_data}{Data Frame; The data required for the severity model}

\item{priors}{BRMS Prior; The set of priors for both the frequency and severity models}

\item{ded_name}{Character; The column name for the deductible/excess/attachment point in the frequency data}

\item{freq_adj_fun}{Character; The Stan function to adjust the frequency mean by. If NULL, the survival function of the severity model at the deductible will be used.}

\item{...}{Additional accepted BRMS fit parameters}
}
\value{
BRMS Fit
}
\description{
This function allows you to create a multivariate Bayesian
frequency-severity model in which the frequency parameter is adjusted
by the survival function of the severity model at the deductible/excess/attachment point.

This allows for both linear and non-linear functions of the variates for
both the frequency and severity model components.
}
\examples{

#### Simulate Frequency Data ####

options(stringsAsFactors = FALSE,
        mc.cores = parallel::detectCores())

#' Assuming one rating factor: region.

set.seed(123456)

# Region Names

regions = c("EMEA", "USC")

# Number of frequency samples

freq_n = 5e3

# Defines a function for lambda

freq_lambda = exp(c(EMEA = 0.5, USC = 1))

# Generate samples for ground-up frequency data

freq_data =
  data.frame(
    pol_id =  seq(freq_n),
    ded = runif(freq_n, 1e3, 5e3),
    lim = runif(freq_n, 50e3, 100e3),
    region = sample(regions, freq_n, replace = T)
  ) \%>\%
  mutate(
    freq_lambda = freq_lambda[region],
    claimcount_fgu =
      rpois(freq_n, freq_lambda)
  )

#### Simulate severity Data ####

mu_vec = c(EMEA = 8, USC = 9)
sigma_vec = exp(c(EMEA = 0, USC = 0.4))

sev_data =
  data.frame(
    ded = rep(freq_data$ded,
              freq_data$claimcount_fgu),
    lim = rep(freq_data$lim,
              freq_data$claimcount_fgu),
    region = rep(freq_data$region,
                 freq_data$claimcount_fgu)
  ) \%>\%
  mutate(
    loss_uncapped =
      unlist(
        lapply(
          seq(freq_n),
          function(i){

            rlnorm(freq_data$claimcount_fgu[i],
                   mu_vec[freq_data$region[i]],
                   sigma_vec[freq_data$region[i]]
            )

          }
        )
      )
  ) \%>\%
  mutate(
    pol_id = rep(seq(freq_n), freq_data$claimcount_fgu)
  ) \%>\%
  filter(
    loss_uncapped > ded
  ) \%>\%
  mutate(
    claim_id = row_number(),
    lim_exceed = as.integer(loss_uncapped >= lim),
    loss = pmin(loss_uncapped, lim)
  )

# Frequency data filtered for losses below the deductible

freq_data_net =
  freq_data \%>\%
  left_join(
    sev_data \%>\%
      group_by(
        pol_id
      ) \%>\%
      summarise(
        claimcount = n()
      ) \%>\%
      ungroup(),
    by = "pol_id"
  ) \%>\%
  mutate(
    claimcount = coalesce(claimcount, 0)
  )

#### Run Model ####

mv_model_fit =
  brms_freq_sev(

    freq_formula =
      bf(claimcount ~ 1 + region),

    sev_formula =
      bf(loss | trunc(lb = ded) + cens(lim_exceed) ~
           1 + region,
         sigma ~ 1 + region
      ),

    freq_family = poisson(),
    sev_family = lognormal(),

    freq_data = freq_data_net,
    sev_data = sev_data,

    priors = c(prior(normal(0, 1),
                     class = Intercept,
                     resp = claimcount),

               prior(normal(0, 1),
                     class = b,
                     resp = claimcount),

               prior(normal(8, 1),
                     class = Intercept,
                     resp = loss),

               prior(lognormal(0, 1),
                     class = Intercept,
                     dpar = sigma,
                     resp = loss),

               prior(normal(0, 1),
                     class = b,
                     dpar = sigma,
                     resp = loss)
    ),

    ded_name = "ded",

    chains = 1,
    iter = 1000,
    warmup = 250,
    refresh = 50,
    control =
      list(adapt_delta = 0.999,
           max_treedepth = 15)
  )

#### Results ####

model_post_samples =
  posterior_samples(
    mv_model_fit
  ) \%>\%
  transmute(
    s1_emea = b_loss_s1_Intercept,
    s1_usc  = b_loss_s1_Intercept +
      b_loss_s1_regionUSC,

    sigma_emea = exp(b_sigma_loss_Intercept),
    sigma_usc  = exp(b_sigma_loss_Intercept +
                       b_sigma_loss_regionUSC),

    f1_emea = b_claimcount_f1_Intercept,
    f1_usc  = b_claimcount_f1_Intercept +
      b_claimcount_f1_regionUSC
  )

model_output =
  model_post_samples \%>\%
  sapply(
    function(x) c(lower = quantile(x, 0.025),
                  mean  = mean(x),
                  upper = quantile(x, 0.975))
  ) \%>\%
  as.data.frame() \%>\%
  bind_rows(
    data.frame(
      s1_emea = 8,
      s1_usc  = 9,

      sigma_emea = exp(0),
      sigma_usc  = exp(0.4),

      f1_emea = 0.5,
      f1_usc  = 1
    )
  )

}
