% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={The Bayesian Actuarial Package},
  pdfauthor={Chris Waller and Zongwen Tan},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{The Bayesian Actuarial Package}
\author{Chris Waller and Zongwen Tan}
\date{2021-08-10}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

\hypertarget{about-the-authors}{%
\chapter{About the authors}\label{about-the-authors}}

\hypertarget{chris-waller}{%
\section{Chris Waller}\label{chris-waller}}

Chris is a qualified Senior Technical Pricing Actuary (FIA) based in London, UK, with a Master's in Mathematics from the University of Warwick. He currently works for American International Group (AIG); developing Bayesian statistical models to price commercial lines insurance across a range of different lines of business (e.g.~Marine, Property, Energy, Financial Lines, Casualty, Aerospace).

He is currently studying Japanese and is working towards taking the Japanese Language Proficiency Test (Level N2).

\textbf{E-Mail}: \href{mailto:chriswaller26@gmail.com}{\nolinkurl{chriswaller26@gmail.com}}

\textbf{LinkedIn}: \url{https://www.linkedin.com/in/chris-waller-mmath-fia-56a40437/}

\textbf{GitHub}: \url{https://github.com/ChrisWaller26}

\hypertarget{zongwen-tan}{%
\section{Zongwen Tan}\label{zongwen-tan}}

\hypertarget{frequency-severity-modelling}{%
\chapter{Frequency-Severity Modelling}\label{frequency-severity-modelling}}

\hypertarget{an-overview-of-bayesian-frequency-severity-modelling}{%
\section{An overview of Bayesian frequency-severity modelling}\label{an-overview-of-bayesian-frequency-severity-modelling}}

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

In the world of insurance, there are a vast array of different lines of business that can be written - all of which can have very different and complex policy and claim structures. As a consequence, different lines of business may require vastly different modelling approaches (different rating factors, use of splines, linear vs non-linear relationships between covariates etc.).

The ultimate goal of any actuary or data scientist when developing a predictive insurance model is to actually predict future claims experience and/or loss development, with a clear, numerical understanding of the uncertainty around these predictions. Achieving this with traditional frequentist methods requires a number of potentially significant implicit assumptions to be made about the distribution of the parameters that form part of the model.

Adopting a Bayesian approach allows the modeller to better understand the uncertainty around their parameter estimates, as well as being able to embed prior knowledge into their calculation to create reasonable predictions - even when good quality data may be scarce.

\hypertarget{the-standard-frequency-severity-model}{%
\subsection{The standard frequency-severity model}\label{the-standard-frequency-severity-model}}

It is commonplace in most insurance pricing and reserving models to separate frequency (the number of claims) and severity (the magnitude of a loss, given there is a one) when fitting a model. This is because they are often driven by quite different phenomena, with different trends, risk factors and loss development patterns. In mathematical notation, we would write this as:

\[S = \sum_{i=1}^{N} X_i\]

where:

\[N = \mbox{Number of losses}\]
\[X_i = \mbox{Severity of } i \mbox{th loss}\]

If all \(X_i\) are assumed to be independent and identically distributed, as well as being independent of the distribution of the number of losses, \(N\), then we can fit these models separately using whatever methods we deem fit and then take samples from them, as required.

This approach is the bread-and-butter for insurance pricing modelling, as it is easy to understand, straightforward to create and not overly computer power-intensive.

\hypertarget{whats-wrong-with-the-standard-approach}{%
\subsection{What's wrong with the standard approach?}\label{whats-wrong-with-the-standard-approach}}

Even if a Bayesian approach is taken to fitting the frequency and severity models, the standard approach does have some very key flaws when modelling a typical book of insurance.

Most insurance policies have some form of deductible, excess or attachment point which removes a company's exposure to very small, attritional losses. In general, losses below this deductible will go unreported, therefore resulting in he severity data being left-truncated.

Moreover, the frequency data set will be impacted, as (all other things being equal) claim counts will be lower if deductibles are higher.

So why is this an issue? Mainly that, given that only the losses above the deductible end up in the severity data (assuming no net zero losses are reported) and that the claim counts in the frequency data are lower when deductibles are higher, then the severity and frequency distribution are not independent, as we first assumed.

In other words, if we have a policy for which the ground-up losses (i.e.~losses assuming no deductible) have a cumulative density function \(F\) and an expected annual ground-up claim count \(\lambda\) then, given the policy has a deductible \(D\):

\[\lambda^* = \lambda \cdot (1-F(D))\]
where \(\lambda^*\) is the average claim count, net of deductible.

A further complication can be that \(D\) may vary on a policy-by-policy basis, so it may not be appropriate or possible to just model \(\lambda^*\) directly.

\hypertarget{overcoming-the-dependance-issue}{%
\subsection{Overcoming the dependance issue}\label{overcoming-the-dependance-issue}}

One of the many great things about the Stan modelling language is that it allows one to optimise their model fitting using likelihoods which could vary on a policy-by-policy basis.

For example, if we create a LogNormal-Poisson model using Stan and we want to deal with this left-censoring issue, we could loop through all the data points and adjust lambda by the survival function of the severity at the deductible for that data point:

\begin{verbatim}
data{

  int N_freq;              // Number of rows in frequency data
  int claim_count[N_freq]; // Observed claim counts
  vector[N_freq] D_freq;   // Deductibles from frequency data
  
  int N_sev;           // Number of rows in severity data
  vector[N_sev] loss;  // Size of loss 
  vector[N_sev] D_sev; // Deductibles from severity data

}

parameters{

  real mu;
  real <lower=0> sigma;
  real <lower=0> lambda;

}

model{

  for(i in 1:N_sev){
  
    target += lognormal_lpdf(loss[i] | mu, sigma) -
      lognormal_lccdf(D_sev[i] | mu, sigma)
  
  }

  for(i in 1:N_freq){

    real lambda_i;

    lambda_i = lambda * (1 - lognormal_cdf(D_freq[i], mu, sigma));
  
    target += poisson_lpmf(claim_count[i] | lambda_i);

  }

}
\end{verbatim}

This method works well when the parameters are constants, but it becomes increasingly more complex when trying to implement additional features like rating factors, splines and non-linear relationships between covariates.

\hypertarget{why-use-brms}{%
\section{Why use BRMS?}\label{why-use-brms}}

One of the beauties of the BRMS package is that it allows

  \bibliography{book.bib,packages.bib}

\end{document}
